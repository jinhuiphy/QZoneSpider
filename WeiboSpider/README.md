# 微博Spider

标签： 爬虫

---

    前面已经陆陆续续实现了QQ空间、豆瓣、知乎以及12306网站的爬虫，但除了知乎外，其他的数据量都算不上很大，于是这次就选择了对微博进行爬取，刚开始实现的功能只是对用户的基本信息进行爬取，例如昵称、ID、粉丝数、关注数等等，后来想针对某一特定微博爬取其中的评论，于是有了进阶版的爬虫，下面对这些代码使用进行简单的描述。

  1. 首先有两个最基本的对象，一个是以用户为对象的类WeiboUser，另一个是以单独某一条评论为对象的类Weibo：

     - WeiboUser：
        - `__init__`:类的初始化
        - `get_username`:获取用户昵称
        - `get_user_info`:获取用户微博总数、关注数以及粉丝数
        - `get_save_weibo`:获取并保存用户微博的ID、微博的内容及对应的发布时间、发布设备、点赞数、转发数、评论数
        - `remove_save`:对数据库里的信息进行去重处理
        - `auto_get`:自动运行上面几个函数，获得用户所有的基本信息
     - Weibo：
        - `__init__`:类的初始化
        - `get_save_comment`:获取确定微博下面的所有评论内容以及评论人昵称和ID
        - `auto_get`:自动运行上面的函数，获得微博的所有评论

  2. 然后是基于这两个最基本的对象对用户的微博以及评论进行爬取，分别有两个单独的脚本来完成，一个是**getWeibo**，另一个是**getComment**：
    - getWeibo：获取用户的所有微博还是比较容易，首先创建个WeiboUser对象，将给定用户的ID、flite、start_page参数传进去，然后调用auto_get()函数即可。对于用户的ID，可以直接去该用户网页上去看，url里面的一串数字即为该用户ID
    - getComment：该脚本里有三个函数，分别是：
        - `save_as_txt`:主要是将数据库内的数据导成txt格式，后期读取方便
        - `getID`:该函数用来批量获取微博所对应的ID，注意这里的ID和用户的ID不同，渣浪的每一条微博其实都有特定的一个ID，根据这个ID可以确定独一无二的微博
        - `main`:main函数就是获取评论的关键步骤了，他会自动判断微博ID是否存在，如果不存在，他会自动从数据库中将微博ID提取出来保存为txt，当然这一步的前提条件是你已经进行了前面获取Weibo的步骤，然后可以通过修改起始位置和终止位置确定要爬取的微博的范围，当然在爬取大量评论的时候最好多准备些cookie和agent，以备渣浪的封禁

  3. 获取完所有的信息后当然是要做分析拉，**EmotionAnalyze**就是用来分析评论做情感分析的，首先我们把所有的评论保存为txt格式，这就要用到**SaveCommentAsTxt**这个函数，然后再调用**EmotAnalyze**函数对评论文本中的每一条进行分析，得到情感值，并将最后得到的情感值以hist的形式画出来；当然还可以使用**getFrequency**函数分析所有评论中出现频次最高的词语，然后绘制成词云




